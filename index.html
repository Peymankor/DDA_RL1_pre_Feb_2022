<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Reinforcement Learning Lecture Series</title>
    <meta charset="utf-8" />
    <meta name="author" content="Peyman Kor" />
    <meta name="date" content="2022-02-06" />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Reinforcement Learning Lecture Series
### Peyman Kor
### 2022-02-06

---






## Stanford Course Winter 2022 CME 241

--

&lt;center&gt;&lt;img src="img/RL_1.png" height="500px" /&gt;&lt;/center&gt;

???
- this a good course, authors has written the book
---

## Schedule


&lt;center&gt;&lt;img src="img/RL_2.png" height="500px" /&gt;&lt;/center&gt;

???
- We will cover untill DP...
- I tend to respect to graduates from Stanfrod
- workload is so much... I do not know students pass three course in 3 month

---

class: middle,center

## Running Example

--

## Bicycle Inventory Management Example

&lt;center&gt;&lt;img src="img/bicycle.jpg" height="400px" /&gt;&lt;/center&gt;

--
???

---
--

- You are store manager of bicycle shop: 

--

&lt;center&gt;&lt;img src="img/bicycle.jpg" height="200px" /&gt;&lt;/center&gt;

--

- Every day you have customers, and limited storage (inventory)

--

-  Fail in meeting customer demand: **lost sell**, keeping a bicycle in storage will have **overnight cost**.

--

- &lt;span style = "color:red;"&gt;What are the optimal orders (number of bicycles) from the supplier every day?&lt;span&gt;

--
???
- It is a good problem, from pedagogy
- It is easy to understadn, no need for technical knowledge
- It is dicrete problem
- Bellman, in his book, The Optimal Inventory Equation
- Promising Slide, can fromulate and solve

---
class: middle,center

# 1. Markov Process

--

# 2. Markov Reward Process

--

# 3. Markov Decision Process

--

# 4. Dynamic Programming Algorithms

--
 
# 5. Approximate Dynamic Programming Algorithms

???

- according to the teacher, buidling block RL problems

---
class: middle,center

# 1. Markov Process

---

-  **Process** is a sequence of **random outcomes** at time variable `\(t=0,1,2,...\)`


--


- **random outcomes**: e.g., price of a derivative, inventory level, portfolio value etc.


--

- The internal representation of the process at time `\(t\)` as the **(random) state** it as `\(S_t\)`.

--

- What we are interested:

`$$\mathbb{P}[S_{t+1}|S_t, S_{t-1},...,S_0]$$`


---

## Example:

- Stock price at time `\(t\)` as `\(X_t\)`. From time step `\(t\)` to time step `\(t+1\)`:

--

- `\(X_t\)` can either go up by `\(1\)` or go down by `\(1\)`, i.e., the only two outcomes for `\(X_{t+1}\)` are `\(X_{t} + 1\)` or `\(X_{t} - 1\)`, according:

--

`$$\mathbb{P}[X_{t+1}=X_t+1]=\frac{1}{1+e^{-\alpha(L-X_t)}}$$`
--

`$$\mathbb{P}[X_{t+1} = X_{t} − 1] = 1 − \mathbb{P}[X_{t+1} = X_t + 1]$$`
???

- be carefull about notaion between X_{t+1} and X_{t} +1

---

- `$$\mathbb{P}[X_{t+1}=X_t+1]=\frac{1}{1+e^{-\alpha(L-X_t)}}$$`

--

- Five traces of stock prices, with `\(S_0=100, L=100,  \alpha= 0.25\)`
&lt;center&gt;&lt;img src="img/process1_mp.png" height="400px" /&gt;&lt;/center&gt;

--
???

- I do not know, what is the mean-reversin, but this seems doing that
- L is the level , or mean
---

- Then, what is the Markov Property of states?

--

- **The future is independent of the past given the present.**

--

- Markov Property of states

`$$\mathbb{P}[S_{t+1}|S_t, S_{t-1}, ..S_0]=\mathbb{P}[S_{t+1}|S_t] \; \; \text{for all} \; t&gt;0$$`
--

- The dynamics of a Markov Process, can fully specified with :

`$$\mathcal{P}(s,s')=\mathbb{P}[S_{t+1}=s'|S_t=s] \; \; \\ \text{for all} \; t&gt;0, \text{for all}\; \; s \in \mathcal{S}-\mathcal{T}, s' \in \mathcal{S}$$`

---

## Formal Definition of Markov Process:

- A countable set of states `\(S\)` (known as the State Space) and a set `\(T ⊆ S\)` (known as
the set of Terminal States).


- A time-indexed sequence of random states `\(S_t \in S\)` for time steps `\(t = 0, 1, 2, \cdots\)`
with each state transition satisfying the Markov Property: `\(P[S_{t+1}|St, S_{t−1},S_0] = P[S_{t+1}|S_t]\)` for all `\(t ≥ 0\)`.

- Termination: If an outcome for `\(S_T\)` (for some time step `\(T\)`) is a state in the set `\(T\)` , then
this sequence outcome terminates at time step `\(T\)`.

---


## Bicycle Inventory Management Example



&lt;center&gt;&lt;img src="img/bicycle.jpg" height="300px" /&gt;&lt;/center&gt;

--

- How to formulate the transition of states.

- **Sequential Uncertainty**

--
???

- we are not talking about decsion or reward, only how staes evolves

---

.pull-left[

&lt;!-- #### It is 6 PM Monday --&gt;

-  `\(\alpha\)`: is the inventory in the store (On-Hand Inventory at 6pm)

- `\(\beta\)`: is the inventory on a truck from the supplier (that you had ordered the previous day) 

{{content}}

]

--

- Observe State `\(S_t: (\alpha, \beta)\)` at 6pm store-closing
- Order Quantity := `\(max(C − (\alpha + \beta), 0)\)`

--

.pull-right[

- Receive inventory at 6am if you had ordered `\(36\)` hrs ago

- Experience random demand `\(i\)` with poisson probabilities:
$$\text{PMF} \; \; f(i)=\frac{e^{-\lambda}\lambda^{i}}{i!} $$
{{content}}

]

--

- Close the store at 6pm
- Observe new state `\(S_{t+1} : (\alpha, \beta)\)`

--

???

- be careful on timing, the left could be monday, right on tuesday
- we check the stae only on 6 pm
-  will have aexample of realization in next slide
- C is the storage, here we consider 2, to make space low


---

.pull-left[

&lt;!-- #### It is 6 PM Monday --&gt;

- Observe State `\(S_0: (\alpha=0, \beta=0)\)`

{{content}}

]
--

- Order Quantity := `\(max(C − (\alpha + \beta), 0)\)`,  `\(A_0= max(2 − (0 + 0), 0)=2\)` 

{{content}}

--

&lt;!-- #### It is 6 AM Tuesday --&gt;

- Receive Inventory at 6am if you had ordered `\(36\)` hrs ago, `\(\beta=0\)`

{{content}}

--

&lt;!-- #### It is 6 8AM Tuesday --&gt;

- Open store between 8AM-6PM,  experience random demand `\(i=1\)`

{{content}}

--


&lt;!-- #### Close the store at 6pm --&gt;

- Observe new state `\(S_{1}: (\alpha, \beta), S_1=(0,2)\)`


--

.pull-right[

&lt;!-- #### It is 6 PM Tuesday --&gt;

- Observe new state `\(S_{1}: (\alpha, \beta), S_1=(0,2)\)`

{{content}}

]
--

- Order Quantity := `\(A_1= max(2 − (2 + 0), 0)=0\)` 

{{content}}

--

&lt;!-- #### It is 6 AM Wenesday --&gt;

- Receive order at 6am if you had ordered `\(36\)` hrs ago, `\(\beta=2\)`

{{content}}

--

&lt;!-- #### It is 6 8AM Wenesday --&gt;

- Open store between 8AM-6PM,  `\(i=2\)`

{{content}}

--


&lt;!-- #### Close the store at 6pm --&gt;

- `\(S_{2}: (\alpha, \beta), S_2=(0,0)\)`


--


---

- Storage capacity at most C: `\(C=2\)`, Poisson parameter: `\(\lambda = 1.0\)`

--

- ###  How states transition happens? - `\(\mathbb{P}[S_{t+1}|S_t] \; \; \text{for all} \; t&gt;0\)` 

&lt;center&gt;&lt;img src="img/md_schema.png" height="450px" /&gt;&lt;/center&gt;

--
???

- uncertainity and states, 
- the poission distribution with gamma 1, probaliy of 0 and 1 has same probality

---

class: middle,center

# 2. Markov Reward Process

--
???

- it was sequential uncertainty not reward, 
- add rewward, because letter we want to use decsions to maximize reward

---

--

- Embellish Inventory Process with **Holding cost** and **Stockout cost**

--

- **Holding cost** of `\(h\)` for each unit that remains overnight.
- Think of this as "interest on inventory", also includes upkeep cost.

--

- **Stockout cost** of `\(p\)` for each unit of "missed demand".
- For each customer demand you could not satisfy with store inventory

--

- Generally, `\(p&gt;&gt;h\)`



---

- **A Markov Reward Process (MRP)** is a Markov Process-

--

- Along with a time-indexed sequence of **Reward** random variables `\(R_t \in \mathcal{D}\)` (a countable subset of `\(\mathbb{R}\)`) 

--

- for time steps `\(t = 1,2,...\)`, satisfying the Markov Property (including **Rewards**): 

--

`$$\mathbb{P}[(R_{t+1}, S_{t+1})|S_t, S_{t−1},...,S_0] = \mathbb{P}[(R_{t+1}, S_{t+1})|S_t]\; \text{for all } t ≥ 0$$`

--

- `\(\mathcal{P}_{R}\)` known as the **Transition Probability Function**:

`$$\mathcal{P}_R(s,r,s') = \mathbb{P}[(R_{t+1} = r, S_{t+1} = s')|S_t = s] \; \text{for all} \; t&gt;0$$`
???

- we are in the case where adding the reward in the transition of states

---

## Reward in Bicycle Inventory Management Example

--

- if `\(\alpha\)` in in `\(S_{t+1}\)` greater than zero,:

- `$$=-h.\alpha$$`

--

- if `\(\alpha\)` in in `\(S_{t+1}\)` is zero:


`$$=-ha-p(\sum_{j=\alpha+\beta+1}^{\infty}f(j)(j-(\alpha+\beta)))$$`

--

???
- paranthezse in the xpected missed demand
- what is the left side of the equaton, for costs

---

- The reward transition function:

--

- `\(h=1, \; \; p=10 , \; \;C=2 , \; \; \lambda=1\)`

--

&lt;center&gt;&lt;img src="img/MRP.png" height="350px" /&gt;&lt;/center&gt;

--

`$$\mathcal{R}_T(s,s')=\mathbb{E}[R_{t+1}|S_{t+1}=s', S_t=s]$$`

--

???

- I need to exaplain it more in next slide
- The left side of the eqution was R(s,s) 

---

- The Data structure in coding:

&lt;center&gt;&lt;img src="img/MRP_annot.png" height="500px" /&gt;&lt;/center&gt;

--

???

- The reason I sho is not show my python code
- how the data structure should look like

---

- What we have from reward modeling:

--

`$$\mathcal{R}_T(s,s')=\mathbb{E}[R_{t+1}|S_{t+1}=s', S_t=s]$$`

--

- What is needed in MRP:

`$$\mathcal{R}(s)=\mathbb{E}[R_{t+1}|S_t=s]=\sum_{s' \in \mathcal{S}}\mathcal{P}(s,s').\mathcal{R}_T(s,s')= \\ \sum_{s' \in \mathcal{S}}\sum_{r \in \mathcal{D}}\mathcal{P}_R(s,r,s').r$$`
--
???
- if we had p_r, we did not had problem,
- but most of time we do not have that,
- rather we have p(s,s') and R_T
- Expectaion sign on left, on right we are taking average

---

&lt;!-- - As you might imagine now, we’d want to identify non-terminal states with large expected returns and those with small - expected returns. This, in fact, is the main problem --&gt;
&lt;!-- - involving a Markov Reward Process - to compute the “Expected Return” associated with --&gt;
&lt;!-- - each non-terminal state in the Markov Reward Process. Formally, we are interested in --&gt;
&lt;!-- - computing the Value Function --&gt;

&lt;!-- - The main purpose of Markov Reward Processes is to calculate how much reward we --&gt;
&lt;!-- - would accumulate (in expectation, from each of the non-terminal states) if we let the Process run indefinitely, bearing - in mind that future rewards need to be discounted appropriately (otherwise the sum of rewards could blow up to ) --&gt;


---



---

### Define Return

`$$G_t=\sum_{i=t+1}^{\infty}\gamma^{i-t-1}R_i = R_{t+1} + \gamma R_{t+2} + \gamma R_{t+3} + \cdots$$`

--

### `\(\gamma \in [0,1)\)` is the discount factor. Why discount?

--

- Avoids infinite returns in cyclic Markov Processes.

--

- If the reward is financial, discounting due to interest rates.

--

- Animal/human behavior prefers immediate reward.

???

- If we randomly experience reward, how much we get
- Say I am bicyle shop owner, if I run indiintely, how much in Average I get

---

### We want to identify non-terminal states with large expected returns and those with small expected returns.

--

- Expected total Reward, Given the State:

--

$$V(s) = \mathbb{E}[G_t|S_t=s] \; \; \text{for all} \; s \in \mathcal{N}, \text{for all} \; \; t=0,1,2,... $$
--

&lt;center&gt;&lt;img src="img/mrpbellmaneq.png" height="300px" /&gt;&lt;/center&gt;

--

???

- Creative piece by Bellman showing this equation has a recursive structure 1957
- we will be looking more on the last equation

---

### Bellman Equation for Markov Reward Processes

`$$V(s) =\mathcal{R}(s) + \gamma\sum_{s' \in \mathcal{N}}^{}\mathcal{P}(s,s').V(s')$$`

--

### Can we solve for V?

`$$\mathbf{V} = \mathbf{R} + \gamma \mathbf{P}·\mathbf{V}$$`
- `\(\mathbf{V}\)` is a column vector of length `\(m\)`, 
- `\(\mathbf{P}\)` is an `\(m \times m\)` matrix, and `\(\mathbf{R}\)` is a column vector of length `\(m\)` (rows/columns corresponding to states in `\(\mathcal{N}\)`)

--

`$$\mathbf{V} = (\mathbf{I}- \gamma \mathbf{P})^{-1}·\mathbf{R}$$`
???
- the equation is in non terminal states
- the undestanding is lets say, we have s and then there only two s1 and s2
- we are touching Bellman equation, but not quite
- We have analytical equation for the value function, in MRP, but we need iterative process
- because the inversion of bic matrix is difficult
- the first alghorithm of the DP, this the prediction
---

- State-Value in Bicycle Inventory Management Example

--

`$$\mathbf{V} = (\mathbf{I}- \gamma \mathbf{P})^{-1}·\mathbf{R}$$`

--

&lt;center&gt;&lt;img src="img/vf_value_det.png" height="200px" /&gt;&lt;/center&gt;

--

- Can we identify the ordering policy that yields the **Optimal State-Value Function**?

- Bicycle Inventory Management: How much to order?

--

???
- Now we have the transition of uncertainity and reward
- we can jump to the decsion
- because decsion want to maximize the obkjective

---

class: middle,center

# 3. Markov Decision Process

---

- A countable set of states S (State Space), random reward `\(R_t\)`

--

- and a countable set of actions `\(A_t \in \mathcal{A}\)`, - **Markov Property**: 

`$$\mathbb{P}[(R_{t+1}, S_{t+1})|(S_t, A_t, S_{t−1}, A_{t−1}, \cdots  ,S_0, A_0)]=$$`
`$$\mathbb{P}[(R_{t+1}, S_{t+1})|(S_t, A_t)] \; \; \text{for all t} ≥ 0$$`
--

- State, Action, Reward, State,...

`$$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,\cdots$$`
--

- A **sequence of decision= policy** 

--

???

- we are adding the left and right side of p(s,s')
- does not seems to call decsion aor decsions
- qute from Ron hOrwad 
- When di(n) has been specified for all i and n, policy has been determined

---

- What are the types of policy in MDP?

--

### Stochastic Policy:

`$$\pi(s,a)=\mathbb{P}[A_t=a|S_t=s]$$`
--

### Deterministic Policy:

`$$\pi(s, \pi_D(s))=1 \; \; \text {and} \; \; \pi(s,a)=0 \; \; \text {for} \; \; a\in \mathcal{A}, \;  a\neq\pi_D(s)$$`

---
name: markov-review

&lt;!-- ### Transition probality function  in Markov Process: --&gt;

&lt;!-- `$$\mathcal{P}(s,s')=\mathbb{P}[S_{t+1}|S_t] \; \; \text{for all} \; t&gt;0$$` --&gt;
&lt;!-- -- --&gt;

&lt;!-- ### Transition probality function  in Markov Reward Process: --&gt;

&lt;!-- `$$\mathcal{P}_R(s,r,s') = \mathbb{P}[(R_{t+1} = r, S_{t+1} = s')|S_t = s] \; \text{for all} \; t = 0,1,2,...$$` --&gt;
--

**Transition Probability function** in Markov Decision Process:

`$$\mathcal{P}_R: \; \mathcal{N}\times\mathcal{A}\times\mathcal{D}\times\mathcal{S} \rightarrow [0,1]$$`
--

`$$\mathcal{P}_R(s,a,r,s')=\mathbb{P}[(R_{t+1}=r, S_{t+1}=s')|(S_t=s, A_t=a)]$$`
--

### State Transition Probability function

`$$\mathcal{P}(s,a,s')=\sum_{r\in \mathcal{D}}{}\mathcal{P}_R(s,a,r,s')$$`
--

### The reward transition function:

`$$\mathcal{R}(s,a)=\mathbb{E}[R_{t+1}|(A_t=a, S_t=s)]$$`
--

---

### MDP State-Value Function Bellman Policy Equation 

--


`$$V^{\pi}(s)= \mathbb{E}_{\pi, \mathcal{P}_R}[G_t|S_t=s] \;  \text{for all} \; s\in\mathcal{N}$$`
--

&lt;!-- `$$V^{\pi}(s)= \sum_{a \in \mathcal{A}}{}\pi(s,a).(\mathcal{R}(s,a) + \gamma\sum_{s'\in \mathcal{N}}\mathcal{P(s,a,s')}. V^{\pi}(s')) \; \; \text{for all} \; s\in\mathcal{N}$$` --&gt;


&lt;!-- - The state transition probability function --&gt;

&lt;!-- `$$\mathcal{P}(s,a,a') = \mathbb{P}[S_{t+1}=s|(S_t=s, A_t=a)]$$` --&gt;

- In Deterministic Policy:

`$$V^{\pi_D}(s)= \mathcal{R}(s,\pi_D(s)) + \gamma\sum_{s'\in \mathcal{N}}\mathcal{P(s,\pi_D(s),s')}. V^{\pi}(s') \; \; \\ \text{for all} \; s\in\mathcal{N}$$`
--


---

### MDP Action-Value Function Bellman Policy Equation 

&lt;!-- `$$Q^{\pi}: \; \mathcal{N}\times\mathcal{A} \rightarrow \mathbb{R}$$` --&gt;
$$Q^{\pi}(s, a) = \mathbb{E}_{\pi,\mathcal{P}_R}[G_t|(S_t = s, A_t = a)] \; \text{for all} \; \; s \in \mathcal{N}, a \in \mathcal{A} $$
--

- In Stochastic Policy:

`$$Q^{\pi}(s,a)= \mathcal{R}(s,a) + \gamma\sum_{s'\in \mathcal{N}}\mathcal{P(s,a,s')}. \sum_{a'\in \mathcal{A}}\pi(s',a') Q^{\pi}(s',a')\\\text{for all} \;s\in\mathcal{N}, a \in \mathcal{A}$$`

---

&lt;!-- The way to interpret Qπ(s, a) is that it’s the Expected Return from a given --&gt;
&lt;!-- non-terminal state s by first taking the action a and subsequently following policy π. With --&gt;
&lt;!-- this interpretation of Qπ(s, a), we can perceive V π(s) as the “weighted average” of Qπ(s, a) --&gt;


---

### MDP State-Value Function Bellman Policy Equation

--

`$$V^{\pi_D}(s)= \mathcal{R}(s,\pi_D(s)) + \gamma\sum_{s'\in \mathcal{N}}\mathcal{P(s,\pi_D(s),s')}. V^{\pi}(s') \; \; \\ \text{for all} \; s\in\mathcal{N}$$`
--

- **Optimal State-Value Function `\(V^*\)`**

$$V^*(s)=\max_{\pi \in \Pi}V^{\pi}(s) \; \; \; \text{for all} \; \; \; s \in \mathcal{N} $$

--

- For each `\(s\)`, maximize `\(V(s)\)` across choices of `\(\pi \in \Pi\)` 

--
---

### MDP State-Value Function Bellman Optimality Equation

--

`$$V^*(s) = \max_{a \in \mathcal{A}} \{\mathcal{R}(s,a)+\gamma\sum_{s'\in\mathcal{N}}\mathcal{p}(s,a,s').V^*(s')\} \; \; \\ \text{for all} \; s \in \mathcal{N}$$`
--

&lt;!-- ### MDP Action-Value Function Bellman Optimality Equation --&gt;


&lt;!-- `$$Q^*(s,a) =  \mathcal{R}(s,a)+\gamma\sum_{s'\in\mathcal{N}}\mathcal{p}(s,a,s'). Q^*(s',a')\} \; \; \\ \text{for all} \; s \in \mathcal{N}, a \in\mathcal{A}$$` --&gt;


---

## Theorem:

### For any (discrete-time, countable-spaces, stationary) MDP:

- There exists an Optimal Policy `\(π^* \in Π\)`, such that `\(V^{π^∗}(s) ≥ V^{π}(s)\)` for all policies `\(π ∈ Π\)` and for all states `\(s ∈ N\)`.

--

- All Optimal Policies achieve the Optimal Value Function, i.e. 
`\(V^{π^∗}(s) = V^{∗}(s)\)` for all `\(s \in N\)` for all Optimal Policies `\(π^{∗}\)`

---
## Conclusion until this point:

--

### Prediction Problem:

`$$\mathbf{V^\pi} = (\mathbf{I_m}- \gamma \mathcal{P^{\pi}})^{-1}·\mathbf{R^{\pi}}$$`

`$$\mathbf{V^\pi} = \mathbf{R^{\pi}} + \gamma \mathcal{P^{\pi}}·\mathbf{V^\pi}$$`
--

### Control Problem:

`$$V^*(s) = \max_{a \in \mathcal{A}} \{\mathcal{R}(s,a)+\gamma\sum_{s'\in\mathcal{N}}\mathcal{p}(s,a,s').V^*(s')\} \; \; \\ \text{for all} \; s \in \mathcal{N}$$`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current% / %total%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(60000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
